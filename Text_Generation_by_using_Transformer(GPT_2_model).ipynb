{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEFZqYtXLXgP"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrPJOisGLdDj"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.9.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhZbflijZJY_"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3IHDMonPMBJ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] accelerate -U\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67ScJJJyYZrF"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import accelerate\n",
        "\n",
        "print(transformers.__version__)\n",
        "print(accelerate.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px-rhf3pEALP"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Important  libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aegFq9GH8Fh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from datasets import Dataset, DatasetDict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Data Set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2VCXQjQKJW3"
      },
      "outputs": [],
      "source": [
        "path_to_file = 'shakespeare.txt'\n",
        "text = open(path_to_file, 'r').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "098RI7Wu4Ssy"
      },
      "source": [
        "# **Text Cleaning**\n",
        "Library Used: re (Regular Expression operations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2shOkVQDKYYV"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'\\r', '', text)\n",
        "    \n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        " \n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        " \n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "   \n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "cleaned_text = clean_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQNMTZs1cffI"
      },
      "source": [
        "# **Preprocessing and Tokenization**\n",
        "Library Used: transformers from Hugging Face\n",
        "\n",
        " Uses GPT2Tokenizer from the transformers library to tokenize the cleaned text and convert tokens to token IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktE_aPEPyZ3B"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbRiGHerFwUf"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGrLbka9dLKY"
      },
      "outputs": [],
      "source": [
        "tokenized_text = tokenizer.tokenize(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGcrmPAxyYVU"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFd7hi4dyh32"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save('shakespeare_token_ids.npy', token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qhf4R-Ayvf9"
      },
      "source": [
        "# **Feature Engineering**\n",
        "Library Used: numpy\n",
        "\n",
        " Uses numpy to create input sequences of token IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-FXKFToyyMn"
      },
      "outputs": [],
      "source": [
        "def create_input_sequences(token_ids, seq_length):\n",
        "    input_sequences = []\n",
        "    for i in range(0, len(token_ids) - seq_length, seq_length):\n",
        "        input_sequences.append(token_ids[i:i + seq_length])\n",
        "    return np.array(input_sequences)\n",
        "\n",
        "seq_length = 128\n",
        "input_sequences = create_input_sequences(token_ids, seq_length)\n",
        "\n",
        "\n",
        "np.save('shakespeare_input_sequences.npy', input_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj6oqJiw5ae8"
      },
      "source": [
        "# **Data Splitting**\n",
        "Library Used: scikit-learn\n",
        "\n",
        " Uses **train_test_split** from **scikit-learn** to split the data into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLdBkXDcVIDD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sequences, val_sequences = train_test_split(input_sequences, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrPGDg7R5xvA"
      },
      "source": [
        "# **Dataset Conversion**\n",
        "Library Used: **datasets** from Hugging Face\n",
        "\n",
        "Uses **Datase**t and **DatasetDict** from the **datasets** library to create datasets for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32w-gtZFeBJY"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_dict({'input_ids': train_sequences.tolist()})\n",
        "val_dataset = Dataset.from_dict({'input_ids': val_sequences.tolist()})\n",
        "\n",
        "\n",
        "datasets = DatasetDict({'train': train_dataset,'validation': val_dataset})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFYZCj506MXo"
      },
      "source": [
        "# **Model Loading and Fine-Tuning**\n",
        "Library Used: **transformers** from Hugging Face\n",
        "\n",
        "Uses **GPT2LMHeadModel**, **DataCollatorForLanguageModeling**, **Trainer**, and **TrainingArguments** from the **transformers** library to load the GPT-2 model and fine-tune it on the Shakespeare dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GetIm2Yo3Dtm"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clnL26uV3pcb"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mERJJ3MaePHz"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./gpt2-shakespeare-finetuned',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=15,  \n",
        "    per_device_train_batch_size=4,  \n",
        "    gradient_accumulation_steps=2, \n",
        "    save_steps=1000, \n",
        "    save_total_limit=3,\n",
        "    logging_steps=200,)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=datasets['train'],\n",
        "    eval_dataset=datasets['validation'])\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaNX43id6snU"
      },
      "source": [
        "# **Text Generation**\n",
        "Library Used: **transformers** from **Hugging Face**, **torch** for handling tensors.\n",
        "\n",
        "Uses the **GPT2LMHeadMode**l and **GPT2Tokenizer** from the **transformers** library along with **torch** for tensor operations to generate text based on a seed input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93JEaEL8F7UQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def generate_text(model, tokenizer, seed_text, max_length=100, temperature=0.7, top_k=50, top_p=0.95):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(device)\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=1.2,  \n",
        "        pad_token_id=tokenizer.eos_token_id)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "seed_text = \"AGRIPPA. My dear friend, I have seen the heavens fall and\"\n",
        "generated_text = generate_text(model.cuda(), tokenizer, seed_text, max_length=100,  temperature=0.7, top_k=40, top_p=0.9)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP9fuUto7rrc"
      },
      "source": [
        "# **Working with Layers**\n",
        "**Embedding Layer**\n",
        "\n",
        "When the input text is tokenized and converted to token IDs, the embedding layer maps these IDs to dense vectors.\n",
        "\n",
        "**Role:** The input_ids are fed into the embedding layer to obtain initial embeddings.\n",
        "\n",
        "**Transformer Layers**\n",
        "\n",
        "The core of GPT-2 consists of multiple transformer layers, each performing two main operations:\n",
        "\n",
        "**1. Self-Attention Mechanism:**\n",
        "\n",
        "This mechanism allows the model to weigh the importance of different tokens in the sequence when producing the next token. Each transformer layer has multiple heads (multi-head self-attention) to capture various relationships in the data.\n",
        "\n",
        "**2. Feed-Forward Network:**\n",
        "\n",
        " After self-attention, the data is passed through a feed-forward neural network for further processing.\n",
        "\n",
        " **Fine-Tuning with Transformer Layers**\n",
        "\n",
        "During fine-tuning, the Trainer class handles the forward and backward passes through these transformer layers.\n",
        "\n",
        "**Forward Pass:**\n",
        "\n",
        " For each batch of data, the input sequences are passed through the embedding layer and then through each of the transformer layers. The model computes the loss based on the difference between the predicted and actual next tokens.\n",
        "\n",
        "**Backward Pass:**\n",
        "\n",
        " The optimizer updates the model parameters in the transformer layers to minimize the loss.\n",
        "\n",
        " **Generating Text with Transformer Layers**\n",
        "\n",
        "When generating text, the model uses the same transformer layers to predict the next token in the sequence.\n",
        "\n",
        "**Self-Attention Mechanism:**\n",
        "\n",
        "In each transformer layer, the self-attention mechanism calculates attention scores for the input sequence.\n",
        "\n",
        "**Feed-Forward Network:**\n",
        "\n",
        "The attention scores are processed through a feed-forward network to produce the next token's embedding.\n",
        "\n",
        "**Output Layer:**\n",
        "\n",
        "The final layer converts these embeddings into probabilities over the vocabulary, from which the most likely next token is sampled."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
